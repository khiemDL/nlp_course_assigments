{"cells":[{"cell_type":"markdown","metadata":{"id":"sbWNBFv5ineh"},"source":["# Word embedding and one-hot encoding\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PfCcod1xoDoF"},"source":["## One-hot encoding\n","\n","> One-hot encoding is the process of turning categorical factors into a numerical structure that machine learning algorithms can readily process. It functions by representing each category in a feature as a binary vector of 1s and 0s, with the vector's size equivalent to the number of potential categories."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tl5VgYs_ipju"},"outputs":[],"source":["data = ['cold', 'cold', 'warm', 'cold', 'hot', 'hot', 'warm', 'cold', 'warm', 'hot']"]},{"cell_type":"markdown","metadata":{"id":"ZVqliCz4njHW"},"source":["### One-hot integer encoding"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1743754827206,"user":{"displayName":"Khiêm Đặng Lê","userId":"14941239270249401110"},"user_tz":-420},"id":"mf3v1gLInkwK","outputId":"aa084fee-71e3-4f7c-f7d9-94320c8b701b"},"outputs":[{"output_type":"stream","name":"stdout","text":["['cold', 'cold', 'warm', 'cold', 'hot', 'hot', 'warm', 'cold', 'warm', 'hot']\n","[0 0 2 0 1 1 2 0 2 1]\n"]}],"source":["import numpy as np\n","from sklearn.preprocessing import LabelEncoder\n","\n","label_encoder = LabelEncoder()\n","integer_encoded = label_encoder.fit_transform(np.array(data))\n","\n","print(data)\n","print(integer_encoded)"]},{"cell_type":"markdown","metadata":{"id":"C_eLAP-Mn9Dp"},"source":["### One-hot binary encoding"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41,"status":"ok","timestamp":1743754827248,"user":{"displayName":"Khiêm Đặng Lê","userId":"14941239270249401110"},"user_tz":-420},"id":"S8zKywxqn7-8","outputId":"3cf4fbab-8eaf-40b8-b61b-ce59fb3b9127"},"outputs":[{"output_type":"stream","name":"stdout","text":["['cold', 'cold', 'warm', 'cold', 'hot', 'hot', 'warm', 'cold', 'warm', 'hot']\n","[[1. 0. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 1.]\n"," [1. 0. 0.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]\n"," [0. 0. 1.]\n"," [1. 0. 0.]\n"," [0. 0. 1.]\n"," [0. 1. 0.]]\n"]}],"source":["import numpy as np\n","from sklearn.preprocessing import OneHotEncoder\n","\n","one_hot_encoder = OneHotEncoder(sparse_output=False)\n","integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n","onehot_encoded_data = one_hot_encoder.fit_transform(integer_encoded)\n","\n","print(data)\n","print(onehot_encoded_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1743754827255,"user":{"displayName":"Khiêm Đặng Lê","userId":"14941239270249401110"},"user_tz":-420},"id":"71WZ7MFMvz63","outputId":"84fbff27-c99b-4d98-e6ca-2f2852bf529b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([['cold'],\n","       ['cold'],\n","       ['warm'],\n","       ['cold'],\n","       ['hot'],\n","       ['hot'],\n","       ['warm'],\n","       ['cold'],\n","       ['warm'],\n","       ['hot']], dtype='<U4')"]},"metadata":{},"execution_count":55}],"source":["np.array(data).reshape(len(data), 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1743754827260,"user":{"displayName":"Khiêm Đặng Lê","userId":"14941239270249401110"},"user_tz":-420},"id":"OQPiI5_YvV-U","outputId":"962355aa-3c5a-48bc-fcf7-e5f2d36f5fe0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1., 0., 0.],\n","       [1., 0., 0.],\n","       [0., 0., 1.],\n","       [1., 0., 0.],\n","       [0., 1., 0.],\n","       [0., 1., 0.],\n","       [0., 0., 1.],\n","       [1., 0., 0.],\n","       [0., 0., 1.],\n","       [0., 1., 0.]])"]},"metadata":{},"execution_count":56}],"source":["test = one_hot_encoder.fit_transform(np.array(data).reshape(len(data), 1))\n","test"]},{"cell_type":"markdown","metadata":{"id":"gj-KxAzsyN67"},"source":["## Problem 1\n","What are the limitations of one-hot encoding?"]},{"cell_type":"markdown","metadata":{"id":"PnEiFUl2w0r3"},"source":["- Không thể hiện được sự giống nhau về mặt ngữ nghĩa của từ. Các từ đều có khoảng cách như nhau so với các từ khác."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1743754827264,"user":{"displayName":"Khiêm Đặng Lê","userId":"14941239270249401110"},"user_tz":-420},"id":"bQlNqKf6yK6g","outputId":"5edddbc2-ee8a-49ce-c275-8f248cb3e894"},"outputs":[{"output_type":"stream","name":"stdout","text":["1.4142135623730951\n","1.4142135623730951\n"]}],"source":["data = ['cold', 'warm', 'hot', 'cold']\n","data = np.array(data)\n","\n","one_hot_encoder = OneHotEncoder(sparse_output=False)\n","integer_encoded = one_hot_encoder.fit_transform(data.reshape(len(data), 1))\n","\n","#So sánh khoảng cách (sự tương đồng) giữa 'cold' và 'warm' so với 'hot'\n","print(np.linalg.norm(integer_encoded[0] - integer_encoded[2]))\n","print(np.linalg.norm(integer_encoded[1] - integer_encoded[2]))"]},{"cell_type":"markdown","metadata":{"id":"1lBJ3yxY0Kma"},"source":["- Không ghi nhận được tầm quan trọng của các từ trong câu\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1743754827274,"user":{"displayName":"Khiêm Đặng Lê","userId":"14941239270249401110"},"user_tz":-420},"id":"WmGqa4Z86Sw7","outputId":"e7ed9a43-120b-41d8-d0fd-c8509e8ea10a"},"outputs":[{"output_type":"stream","name":"stdout","text":["[array(['cold', 'hot', 'is', 'the', 'warm', 'weather'], dtype='<U7')]\n"]},{"output_type":"execute_result","data":{"text/plain":["array([[0., 0., 0., 1., 0., 0.],\n","       [0., 0., 0., 0., 0., 1.],\n","       [0., 0., 1., 0., 0., 0.],\n","       [0., 1., 0., 0., 0., 0.]])"]},"metadata":{},"execution_count":58}],"source":["data = ['cold', 'warm', 'hot', 'the', 'weather', 'is']\n","data = np.array(data)\n","\n","one_hot_encoder = OneHotEncoder(sparse_output=False)\n","one_hot_encoder.fit(data.reshape(len(data), 1))\n","print(one_hot_encoder.categories_)\n","\n","sentence = ['the', 'weather', 'is', 'hot']\n","sentence = np.array(sentence).reshape(len(sentence), 1)\n","onehot_encoded_data = one_hot_encoder.transform(sentence)\n","onehot_encoded_data\n"]},{"cell_type":"markdown","metadata":{"id":"ARJ3JI1L6VtJ"},"source":["- Bị giới hạn trong bộ từ vựng ban đầu và gặp khó khăn khi thêm từ mới."]},{"cell_type":"markdown","metadata":{"id":"o7OsPVqbrbqr"},"source":["## Word embedding\n","\n","ELI5 for word embeddings\n","> The word embeddings can be thought of as a child’s understanding of the words. Initially, the word embeddings are randomly initialized and they don’t make any sense, just like the baby has no understanding of different words. It’s only after the model has started getting trained, the word vectors/embeddings start to capture the meaning of the words, just like the baby hears and learns different words.\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"df8-lpXbtHe2"},"outputs":[],"source":["import torch\n","from torch import nn\n","from torch.nn import functional as F"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W2EvpsPVvgmW"},"outputs":[],"source":["import pandas as pd\n","\n","corpus = [\n","    'This is the first document.',\n","    'This document is the second document.',\n","    'And this is the third one.',\n","    'Is this the first document?',\n","]"]},{"cell_type":"markdown","metadata":{"id":"UfvwcF5Uz_-A"},"source":["### Unigram transformation"]},{"cell_type":"markdown","metadata":{"id":"w303qoG_2pkh"},"source":["#### Giải thích về thư viện typing.list"]},{"cell_type":"markdown","metadata":{"id":"-9kWu9CM2szR"},"source":["Okay, let's discuss why List from the typing module is used in this case instead of the built-in list.\n","\n","Why typing.List?\n","In Python, type hints were introduced with PEP 484 to provide static typing capabilities. The typing module is a core part of this, offering type annotations like List, Dict, Tuple, etc.\n","\n","Here's why typing.List is preferred over the built-in list in this scenario:\n","\n","Type Clarity and Readability: Using typing.List makes the code more readable and explicit about the expected data types. It clearly communicates that the document argument is expected to be a list of strings and the function will return a list of strings. This enhances code understanding for both the developer and other readers.\n","\n","Static Analysis and Error Detection: Static type checkers like MyPy can use type hints to analyze your code and detect potential type errors before runtime. This helps catch bugs early in the development process. If you used the built-in list without specifying the type of elements it contains, static analysis tools wouldn't be as effective in identifying potential type-related issues.\n","\n","Improved Code Maintainability: Type hints make it easier to maintain and refactor code. When you or someone else needs to modify the code in the future, the type hints serve as documentation, providing clear guidance on the expected data types. This reduces the risk of introducing unintended type errors during code changes.\n","\n","Consistency with Modern Python Practices: Using type hints is considered a best practice in modern Python development. It aligns with the trend of writing more robust and maintainable code.\n","\n","Why not just list?\n","While the built-in list would work functionally, it lacks the type information that typing.List provides. Using just list would be like saying \"this variable is a list,\" but not specifying what kind of items the list should contain.\n","\n","In summary:\n","\n","typing.List enhances code readability, enables static analysis for error detection, improves maintainability, and aligns with modern Python best practices.\n","Using the built-in list without type information reduces the benefits of static typing and can make your code less clear and harder to maintain in the long run.\n","Therefore, even though the built-in list might work in this specific instance, typing.List is the preferred and more robust choice for providing type hints and ensuring code quality. I hope this explanation is helpful!"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1743754827376,"user":{"displayName":"Khiêm Đặng Lê","userId":"14941239270249401110"},"user_tz":-420},"id":"6y4BIvbz3DZY","outputId":"0819a206-50e0-41cb-c21a-e1e508e550af"},"outputs":[{"output_type":"stream","name":"stdout","text":["Help on function ngrams in module nltk.util:\n","\n","ngrams(sequence, n, **kwargs)\n","    Return the ngrams generated from a sequence of items, as an iterator.\n","    For example:\n","    \n","        >>> from nltk.util import ngrams\n","        >>> list(ngrams([1,2,3,4,5], 3))\n","        [(1, 2, 3), (2, 3, 4), (3, 4, 5)]\n","    \n","    Wrap with list for a list version of this function.  Set pad_left\n","    or pad_right to true in order to get additional ngrams:\n","    \n","        >>> list(ngrams([1,2,3,4,5], 2, pad_right=True))\n","        [(1, 2), (2, 3), (3, 4), (4, 5), (5, None)]\n","        >>> list(ngrams([1,2,3,4,5], 2, pad_right=True, right_pad_symbol='</s>'))\n","        [(1, 2), (2, 3), (3, 4), (4, 5), (5, '</s>')]\n","        >>> list(ngrams([1,2,3,4,5], 2, pad_left=True, left_pad_symbol='<s>'))\n","        [('<s>', 1), (1, 2), (2, 3), (3, 4), (4, 5)]\n","        >>> list(ngrams([1,2,3,4,5], 2, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))\n","        [('<s>', 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, '</s>')]\n","    \n","    \n","    :param sequence: the source data to be converted into ngrams\n","    :type sequence: sequence or iter\n","    :param n: the degree of the ngrams\n","    :type n: int\n","    :param pad_left: whether the ngrams should be left-padded\n","    :type pad_left: bool\n","    :param pad_right: whether the ngrams should be right-padded\n","    :type pad_right: bool\n","    :param left_pad_symbol: the symbol to use for left padding (default is None)\n","    :type left_pad_symbol: any\n","    :param right_pad_symbol: the symbol to use for right padding (default is None)\n","    :type right_pad_symbol: any\n","    :rtype: sequence or iter\n","\n"]}],"source":["from nltk import ngrams\n","help(ngrams)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1743754827381,"user":{"displayName":"Khiêm Đặng Lê","userId":"14941239270249401110"},"user_tz":-420},"id":"QnpRB6ct3mRq","outputId":"cb705be6-38a6-447b-9b0c-0bf786aa7805"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["4"]},"metadata":{},"execution_count":62}],"source":["len(corpus)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1743754827388,"user":{"displayName":"Khiêm Đặng Lê","userId":"14941239270249401110"},"user_tz":-420},"id":"0jBuO8sL3UJ2","outputId":"da76e90e-0cb6-4f24-fbc2-dc7f2c1cc3b9"},"outputs":[{"output_type":"stream","name":"stdout","text":["('This', 'is')\n","('is', 'the')\n","('the', 'first')\n","('first', 'document.')\n"]}],"source":["for gram in ngrams(corpus[0].split(' '), n = 2):\n","  print(gram)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1743754827409,"user":{"displayName":"Khiêm Đặng Lê","userId":"14941239270249401110"},"user_tz":-420},"id":"S5HQu2RJ5yyC","outputId":"33423f7f-c436-4aaa-ad00-f393e4cda756"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['This is', 'is the', 'the first', 'first document.']"]},"metadata":{},"execution_count":64}],"source":["[' '.join(gram) for gram in ngrams(corpus[0].split(' '), n = 2)]"]},{"cell_type":"markdown","metadata":{"id":"EVPdkHR64Adq"},"source":["- Cần truyền vào một list"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1743754827415,"user":{"displayName":"Khiêm Đặng Lê","userId":"14941239270249401110"},"user_tz":-420},"id":"yVJ9gBjg4Laa","outputId":"c324bc6a-07b0-4727-86da-f121f68d1780"},"outputs":[{"output_type":"stream","name":"stdout","text":["This is the first document.\n","This document is the second document.\n","And this is the third one.\n","Is this the first document?\n"]}],"source":["for sentence in corpus:\n","  print(sentence)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vPNwu0lWymvJ"},"outputs":[],"source":["from nltk import ngrams\n","from typing import List\n","\n","def ngrams_transform(document: List[str],\n","                     n_gram: int) -> List[str]:\n","    \"\"\"\n","    N-grams transformations for a given text\n","\n","    Args:\n","    document (List[str]) -- The document to-be-processed\n","    n_gram   (int)       -- Number of grams\n","\n","    Returns:\n","    A list of string after n-grams processed\n","    \"\"\"\n","\n","    ### START YOUR CODE HERE ###\n","    result = []\n","    for sentence in document:\n","      result += [' '.join(gram) for gram in ngrams(sentence.split(' '), n = n_gram)]\n","\n","    return result\n","    ### END YOUR CODE HERE ###"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1743754827508,"user":{"displayName":"Khiêm Đặng Lê","userId":"14941239270249401110"},"user_tz":-420},"id":"PKPIUAu_-gX5","outputId":"c0d62fe9-e13b-4592-b2cf-e7014d5cf2ec"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['This',\n"," 'is',\n"," 'the',\n"," 'first',\n"," 'document.',\n"," 'This',\n"," 'document',\n"," 'is',\n"," 'the',\n"," 'second',\n"," 'document.',\n"," 'And',\n"," 'this',\n"," 'is',\n"," 'the',\n"," 'third',\n"," 'one.',\n"," 'Is',\n"," 'this',\n"," 'the',\n"," 'first',\n"," 'document?']"]},"metadata":{},"execution_count":67}],"source":["n_grams_list = ngrams_transform(corpus,\n","                                n_gram=1)\n","n_grams_list"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1743754827514,"user":{"displayName":"Khiêm Đặng Lê","userId":"14941239270249401110"},"user_tz":-420},"id":"w99wG6cn0Dap","outputId":"f680273c-f0a0-48dd-e61d-e937c412fd08"},"outputs":[{"output_type":"stream","name":"stdout","text":["Example text tensor: tensor([ 2,  7, 10,  6,  4,  2,  3,  7, 10,  9,  4,  0, 12,  7, 10, 11,  8,  1,\n","        12, 10,  6,  5])\n","Shape of example text tensor: torch.Size([22])\n"]}],"source":["# Integer label for the given corpus\n","label_encoder = LabelEncoder()\n","corpus_vector = label_encoder.fit_transform(np.array(n_grams_list))\n","\n","# Tensorize the input vector\n","example_text_tensor = torch.Tensor(corpus_vector).to(dtype=torch.long)\n","print(f\"Example text tensor: {example_text_tensor}\")\n","print(f\"Shape of example text tensor: {example_text_tensor.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"0uVmzQtLvs66"},"source":["### Create an example for embedding function to map from a word dimension to a lower dimensional space"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OJTdk0_drc0L"},"outputs":[],"source":["num_vocab = 22 # number of vocabulary\n","num_dimension = 50 # dimensional embeddings\n","\n","# Declare the mapping function\n","example_embedding_function = nn.Embedding(num_vocab, num_dimension)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1743754827564,"user":{"displayName":"Khiêm Đặng Lê","userId":"14941239270249401110"},"user_tz":-420},"id":"ZmTjA3-4uGKG","outputId":"984c2193-b046-410b-eda3-aa99f4ef4c72"},"outputs":[{"output_type":"stream","name":"stdout","text":["Embedding shape: torch.Size([22, 50])\n"]}],"source":["example_output_tensor = example_embedding_function(example_text_tensor)\n","print(f\"Embedding shape: {example_output_tensor.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":50,"status":"ok","timestamp":1743754827615,"user":{"displayName":"Khiêm Đặng Lê","userId":"14941239270249401110"},"user_tz":-420},"id":"KLQNSdVMylu2","outputId":"fff5f499-9a7e-40c6-a583-543277938108"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-2.0442,  1.5012, -1.4179,  ...,  0.1615, -0.4502, -1.6173],\n","        [ 1.2046,  0.3967,  0.8174,  ...,  0.3511,  2.1552,  0.0819],\n","        [ 1.7798, -1.6000,  0.1290,  ..., -0.6039, -1.3091,  0.5135],\n","        ...,\n","        [ 1.7798, -1.6000,  0.1290,  ..., -0.6039, -1.3091,  0.5135],\n","        [ 0.3608,  1.5465, -0.4711,  ..., -1.3572, -1.3828, -0.1157],\n","        [ 1.7119,  0.6990, -0.4768,  ...,  0.4151, -0.1432, -0.0275]],\n","       grad_fn=<EmbeddingBackward0>)"]},"metadata":{},"execution_count":71}],"source":["# Print embedding example\n","example_output_tensor"]},{"cell_type":"markdown","metadata":{"id":"bQPQFfOu0yFA"},"source":["#### Giải thích về các giá trị được khởi tạo"]},{"cell_type":"markdown","metadata":{"id":"eIIWf_d401Xv"},"source":["Trong thư viện `torch.nn.Embedding`, các giá trị được khởi tạo ngẫu nhiên từ phân phối chuẩn N(0, 1). Điều này được thực hiện thông qua hàm `torch.nn.init.normal_` với các giá trị mặc định là `mean=0` và `std=1`.\n","\n","Dưới đây là đoạn mã trong tệp `embedding.cpp` và `sparse.py` của thư viện PyTorch mà bạn có thể tham khảo:\n","\n","```c++ name=torch/csrc/api/src/nn/modules/embedding.cpp\n","void EmbeddingImpl::reset_parameters() {\n","  torch::nn::init::normal_(weight); // Khởi tạo với phân phối chuẩn N(0, 1)\n","  if (options.padding_idx().has_value()) {\n","    torch::NoGradGuard no_grad;\n","    weight[*options.padding_idx()].fill_(0);\n","  }\n","}\n","```\n","\n","```python name=torch/nn/modules/sparse.py\n","def reset_parameters(self) -> None:\n","    init.normal_(self.weight) # Khởi tạo với phân phối chuẩn N(0, 1)\n","    self._fill_padding_idx_with_zero()\n","```\n","\n","Như vậy, khi bạn khởi tạo một đối tượng `nn.Embedding`, các giá trị trong ma trận trọng số sẽ được khởi tạo ngẫu nhiên theo phân phối chuẩn N(0, 1)."]},{"cell_type":"markdown","metadata":{"id":"oElUUcX2ZyDX"},"source":["# Word2vec\n","\n","\n","* Word2vec is a **class of models** that represents a word in a large text corpus as a vector in n-dimensional space(or n-dimensional feature space) bringing similar words closer to each other.\n","\n","\n","\n","* Word2vec is a simple yet popular model to construct representating embedding for words from a representation space to a much lower dimensional space (compared to the respective number of words in a dictionary).\n","\n","\n","\n","* Word2Vec has two neural network-based variants, which are:\n","\n","    * Continuous Bag of Words (CBOW)\n","    * Skip-gram.\n","![](https://kavita-ganesan.com/wp-content/uploads/skipgram-vs-cbow-continuous-bag-of-words-word2vec-word-representation-2048x1075.png)\n"]},{"cell_type":"markdown","metadata":{"id":"KwAVXC0V8vWA"},"source":["## Continuous Bag of words (CBOW)\n","\n","* The Continuous Bag-of-Words model (CBOW) is frequently used in NLP deep learning. It is a model that tries to predict words given the context of a few words before and a few words after the target word. This is distinct from language modeling, since CBOW is not sequential and does not have to be probabilistic. Typically, CBOW is used to quickly train word embeddings, and these embeddings are used to initialize the embeddings of some more complicated model. Usually, this is referred to as pretraining embeddings. It almost always helps performance a couple of percent.\n","\n","* CBOW is modelled as follows:\n","    * Given a target word $w_i$ and an $N$ context window on each side, $w_{i-1}, \\cdots, w_{i-N}$ and $w_{i+1},\\cdots, w_{i+N}$, referring to all context words collectively as $C$.\n","\n","    * CBOW tries to minimize the objective function:\n","\n","$$\n","-\\log p(w_i|C) = -\\log\\text{Softmax}\\left(A\\left(\\sum_{w\\in C}q_w\\right)+b\\right)\n","$$\n","\n","where $q_w$ is the embedding of word $w$."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1743754827632,"user":{"displayName":"Khiêm Đặng Lê","userId":"14941239270249401110"},"user_tz":-420},"id":"mSvo8dncDUvv","outputId":"34630cd6-d77d-4c00-b949-ffce91d81fc8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["62"]},"metadata":{},"execution_count":72}],"source":["# N = 2 according to the definition\n","CONTEXT_SIZE = 2\n","\n","corpus = \"\"\"We are about to study the idea of a computational process.\n","Computational processes are abstract beings that inhabit computers.\n","As they evolve, processes manipulate other abstract things called data.\n","The evolution of a process is directed by a pattern of rules\n","called a program. People create programs to direct processes. In effect,\n","we conjure the spirits of the computer with our spells.\"\"\"\n","\n","corpus = corpus.split()\n","len(corpus)"]},{"cell_type":"markdown","metadata":{"id":"_3MOE0WmFBd-"},"source":["### Create an integer mapping"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":81,"status":"ok","timestamp":1743754827714,"user":{"displayName":"Khiêm Đặng Lê","userId":"14941239270249401110"},"user_tz":-420},"id":"zj1j7Cef2YTr","outputId":"26e1ed0b-bae9-41f9-da7a-52858a961e3b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["49"]},"metadata":{},"execution_count":73}],"source":["len(set(corpus))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1743754827830,"user":{"displayName":"Khiêm Đặng Lê","userId":"14941239270249401110"},"user_tz":-420},"id":"0g4eHuiF8yOj","outputId":"0983d3f2-1bc1-4512-fec1-4c076a934ac3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'things': 0,\n"," 'are': 1,\n"," 'called': 2,\n"," 'create': 3,\n"," 'We': 4,\n"," 'Computational': 5,\n"," 'that': 6,\n"," 'pattern': 7,\n"," 'spells.': 8,\n"," 'a': 9,\n"," 'about': 10,\n"," 'evolution': 11,\n"," 'other': 12,\n"," 'rules': 13,\n"," 'The': 14,\n"," 'the': 15,\n"," 'our': 16,\n"," 'People': 17,\n"," 'by': 18,\n"," 'we': 19,\n"," 'beings': 20,\n"," 'effect,': 21,\n"," 'to': 22,\n"," 'of': 23,\n"," 'is': 24,\n"," 'conjure': 25,\n"," 'processes.': 26,\n"," 'with': 27,\n"," 'In': 28,\n"," 'inhabit': 29,\n"," 'idea': 30,\n"," 'computational': 31,\n"," 'direct': 32,\n"," 'data.': 33,\n"," 'directed': 34,\n"," 'processes': 35,\n"," 'abstract': 36,\n"," 'study': 37,\n"," 'program.': 38,\n"," 'evolve,': 39,\n"," 'programs': 40,\n"," 'manipulate': 41,\n"," 'computer': 42,\n"," 'process': 43,\n"," 'As': 44,\n"," 'spirits': 45,\n"," 'they': 46,\n"," 'computers.': 47,\n"," 'process.': 48}"]},"metadata":{},"execution_count":74}],"source":["vocab = set(corpus)\n","vocab_size = len(vocab)\n","\n","# Integer word mapping\n","word_to_idx = {word: i for i, word in enumerate(vocab)}\n","word_to_idx"]},{"cell_type":"markdown","metadata":{"id":"sqrPWRsTE5zA"},"source":["### Build context according to the given corpus"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":67,"status":"ok","timestamp":1743754827897,"user":{"displayName":"Khiêm Đặng Lê","userId":"14941239270249401110"},"user_tz":-420},"id":"EgBAHKy5CoJ7","outputId":"1768f7ec-7219-4aed-8dff-ad1c7398856c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(['are', 'We', 'to', 'study'], 'about'),\n"," (['about', 'are', 'study', 'the'], 'to'),\n"," (['to', 'about', 'the', 'idea'], 'study'),\n"," (['study', 'to', 'idea', 'of'], 'the'),\n"," (['the', 'study', 'of', 'a'], 'idea'),\n"," (['idea', 'the', 'a', 'computational'], 'of'),\n"," (['of', 'idea', 'computational', 'process.'], 'a'),\n"," (['a', 'of', 'process.', 'Computational'], 'computational'),\n"," (['computational', 'a', 'Computational', 'processes'], 'process.'),\n"," (['process.', 'computational', 'processes', 'are'], 'Computational'),\n"," (['Computational', 'process.', 'are', 'abstract'], 'processes'),\n"," (['processes', 'Computational', 'abstract', 'beings'], 'are'),\n"," (['are', 'processes', 'beings', 'that'], 'abstract'),\n"," (['abstract', 'are', 'that', 'inhabit'], 'beings'),\n"," (['beings', 'abstract', 'inhabit', 'computers.'], 'that'),\n"," (['that', 'beings', 'computers.', 'As'], 'inhabit'),\n"," (['inhabit', 'that', 'As', 'they'], 'computers.'),\n"," (['computers.', 'inhabit', 'they', 'evolve,'], 'As'),\n"," (['As', 'computers.', 'evolve,', 'processes'], 'they'),\n"," (['they', 'As', 'processes', 'manipulate'], 'evolve,'),\n"," (['evolve,', 'they', 'manipulate', 'other'], 'processes'),\n"," (['processes', 'evolve,', 'other', 'abstract'], 'manipulate'),\n"," (['manipulate', 'processes', 'abstract', 'things'], 'other'),\n"," (['other', 'manipulate', 'things', 'called'], 'abstract'),\n"," (['abstract', 'other', 'called', 'data.'], 'things'),\n"," (['things', 'abstract', 'data.', 'The'], 'called'),\n"," (['called', 'things', 'The', 'evolution'], 'data.'),\n"," (['data.', 'called', 'evolution', 'of'], 'The'),\n"," (['The', 'data.', 'of', 'a'], 'evolution'),\n"," (['evolution', 'The', 'a', 'process'], 'of'),\n"," (['of', 'evolution', 'process', 'is'], 'a'),\n"," (['a', 'of', 'is', 'directed'], 'process'),\n"," (['process', 'a', 'directed', 'by'], 'is'),\n"," (['is', 'process', 'by', 'a'], 'directed'),\n"," (['directed', 'is', 'a', 'pattern'], 'by'),\n"," (['by', 'directed', 'pattern', 'of'], 'a'),\n"," (['a', 'by', 'of', 'rules'], 'pattern'),\n"," (['pattern', 'a', 'rules', 'called'], 'of'),\n"," (['of', 'pattern', 'called', 'a'], 'rules'),\n"," (['rules', 'of', 'a', 'program.'], 'called'),\n"," (['called', 'rules', 'program.', 'People'], 'a'),\n"," (['a', 'called', 'People', 'create'], 'program.'),\n"," (['program.', 'a', 'create', 'programs'], 'People'),\n"," (['People', 'program.', 'programs', 'to'], 'create'),\n"," (['create', 'People', 'to', 'direct'], 'programs'),\n"," (['programs', 'create', 'direct', 'processes.'], 'to'),\n"," (['to', 'programs', 'processes.', 'In'], 'direct'),\n"," (['direct', 'to', 'In', 'effect,'], 'processes.'),\n"," (['processes.', 'direct', 'effect,', 'we'], 'In'),\n"," (['In', 'processes.', 'we', 'conjure'], 'effect,'),\n"," (['effect,', 'In', 'conjure', 'the'], 'we'),\n"," (['we', 'effect,', 'the', 'spirits'], 'conjure'),\n"," (['conjure', 'we', 'spirits', 'of'], 'the'),\n"," (['the', 'conjure', 'of', 'the'], 'spirits'),\n"," (['spirits', 'the', 'the', 'computer'], 'of'),\n"," (['of', 'spirits', 'computer', 'with'], 'the'),\n"," (['the', 'of', 'with', 'our'], 'computer'),\n"," (['computer', 'the', 'our', 'spells.'], 'with')]"]},"metadata":{},"execution_count":75}],"source":["data = []\n","\n","for i in range(CONTEXT_SIZE, len(corpus) - CONTEXT_SIZE):\n","    context = (\n","        [corpus[i - j - 1] for j in range(CONTEXT_SIZE)]\n","        + [corpus[i + j + 1] for j in range(CONTEXT_SIZE)]\n","    )\n","    target = corpus[i]\n","    data.append((context, target))\n","\n","data"]},{"cell_type":"markdown","metadata":{"id":"EbFTMNAGWygb"},"source":["### Problem 2\n","Name at least 2 limitations at this context construction step? Explain your answers."]},{"cell_type":"markdown","metadata":{"id":"0CNZeFSO8fEw"},"source":["- Quá trình trên chưa xử lý các từ bị trùng lặp. Ví dụ, từ 'processes' xuất hiện ở giữa câu khác với từ đó ở cuối câu vì dấu chấm: 'processes.'. Do đó, trong bộ voca xuất hiện cả hai từ trên. Hay từ 'computational' khi xuất hiện ở đầu câu thì được viết hoa chữ cái đầu tiên."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1743754827898,"user":{"displayName":"Khiêm Đặng Lê","userId":"14941239270249401110"},"user_tz":-420},"id":"hdL1rZpi-Uxl","outputId":"84dfd2ed-6b1a-4b57-e04d-b9fb8fa3cacb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":76}],"source":["word_to_idx['Computational'] != word_to_idx['computational']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1743754827899,"user":{"displayName":"Khiêm Đặng Lê","userId":"14941239270249401110"},"user_tz":-420},"id":"CEHd21Yh-nq6","outputId":"7eab61a6-a482-418f-e655-1a4285611251"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":77}],"source":["word_to_idx['processes.'] != word_to_idx['processes']"]},{"cell_type":"markdown","metadata":{"id":"SewS_eXT-3WN"},"source":["- Tập training trên bao gồm các bộ từ nằm giữa 2 câu. Điều này không thể hiện được bối cảnh của các từ context và target."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1743754827909,"user":{"displayName":"Khiêm Đặng Lê","userId":"14941239270249401110"},"user_tz":-420},"id":"CI6Ej3APCOfW","outputId":"daca3e3f-9f7f-4b90-9ef3-e5dd02ffe2d9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(['direct', 'to', 'In', 'effect,'], 'processes.')"]},"metadata":{},"execution_count":78}],"source":["# Xét 2 câu sau trong corpus\n","# People create programs to direct processes. In effect, we conjure the spirits of the computer with our spells.\n","data[-11]"]},{"cell_type":"markdown","metadata":{"id":"RYMlC5FSHSqP"},"source":["### Vectorize context"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9aRCDPmYEnMg"},"outputs":[],"source":["def make_context_vector(context: List[str],\n","                        word_to_idx: dict) -> torch.Tensor:\n","    \"\"\"\n","    Function to map a word context vector into a torch tensor\n","\n","    Args:\n","    context (List[str]) -- A context (including individual n-grams tokens)\n","    word_to_idx (dict)  -- A function to map a word into its respective integer\n","\n","    Returns:\n","    A pytorch tensor including a list of mapped word\n","\n","    Example:\n","    ['are', 'We', 'to', 'study'] --> tensor([40, 22, 27, 47])\n","    \"\"\"\n","\n","    ### START YOUR CODE HERE ###\n","    return torch.tensor([word_to_idx.get(word) for word in context])\n","    ### END YOUR CODE HERE ###"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1743754827911,"user":{"displayName":"Khiêm Đặng Lê","userId":"14941239270249401110"},"user_tz":-420},"id":"Fmf-xZkEFOfZ","outputId":"9cd4bc2e-e9dc-4918-b8c1-b4d32834df47"},"outputs":[{"output_type":"stream","name":"stdout","text":["Example sample:  ['are', 'We', 'to', 'study']\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([ 1,  4, 22, 37])"]},"metadata":{},"execution_count":80}],"source":["# Functional test\n","print(\"Example sample: \", data[0][0])\n","make_context_vector(data[0][0], word_to_idx)"]},{"cell_type":"markdown","metadata":{"id":"-h_4P68vHVOW"},"source":["### CBOW model implementation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ViLponY1HZoD"},"outputs":[],"source":["class CBOW(nn.Module):\n","    def __init__(self,\n","                 vocab_size: int,\n","                 embed_dim: int) -> None:\n","        \"\"\"\n","        Model constructor\n","        \"\"\"\n","        super().__init__()\n","\n","        self.vocab_size = vocab_size\n","        self.embed_dim = embed_dim\n","\n","        self.embedding_layer = nn.Embedding(vocab_size, embed_dim)\n","        self.linear_layer = nn.Linear(embed_dim, vocab_size)\n","\n","        # Neural weight initialization\n","        nn.init.xavier_normal_(self.embedding_layer.weight)\n","        nn.init.xavier_normal_(self.linear_layer.weight)\n","\n","    def forward(self, inputs):\n","        \"\"\"\n","        Function to conduct forward passing\n","        \"\"\"\n","        embedding = self.embedding_layer(inputs)\n","        embedding = torch.sum(embedding, dim=1)\n","        output = self.linear_layer(embedding)\n","        output_softmax = F.log_softmax(output, dim=1)\n","        return output_softmax"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1743754827919,"user":{"displayName":"Khiêm Đặng Lê","userId":"14941239270249401110"},"user_tz":-420},"id":"3_vTUt8aJiEn","outputId":"efc1f996-be2a-42a0-8229-4c71a901f3fe"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["CBOW(\n","  (embedding_layer): Embedding(49, 10)\n","  (linear_layer): Linear(in_features=10, out_features=49, bias=True)\n",")"]},"metadata":{},"execution_count":82}],"source":["cbow_model = CBOW(vocab_size=vocab_size,\n","                  embed_dim=10)\n","\n","# Enable gradient for model training\n","cbow_model.train()\n","cbow_model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1743754827926,"user":{"displayName":"Khiêm Đặng Lê","userId":"14941239270249401110"},"user_tz":-420},"id":"-TqG6aFV2j01","outputId":"e9725a71-c4a2-4385-95d7-f085b195f3d2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-3.4890, -4.3462, -4.2833, -4.0922, -3.6406, -4.1345, -3.7928, -3.9724,\n","         -4.4476, -4.0507, -4.4788, -4.0544, -4.0365, -4.0280, -3.8548, -3.9399,\n","         -3.6291, -3.9395, -3.6216, -3.7341, -4.0825, -3.5447, -3.8730, -3.7023,\n","         -3.8599, -3.8637, -3.6783, -3.9750, -3.8881, -4.1699, -4.1348, -4.3463,\n","         -3.6063, -3.7652, -3.7825, -4.0533, -3.7050, -4.2818, -3.4532, -4.0201,\n","         -3.9828, -4.3346, -4.5141, -3.7451, -3.5092, -3.8068, -3.5355, -3.7247,\n","         -3.9387]], grad_fn=<LogSoftmaxBackward0>)"]},"metadata":{},"execution_count":83}],"source":["# prompt: run 1 forward pass with context_example on cbow_model\n","\n","# Assuming cbow_model and other necessary variables are defined as in the provided code.\n","context_example = data[1][0]\n","integer_context = make_context_vector(context_example, word_to_idx)\n","output = cbow_model(integer_context.unsqueeze(0)) # Add a batch dimension\n","output\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1743754827930,"user":{"displayName":"Khiêm Đặng Lê","userId":"14941239270249401110"},"user_tz":-420},"id":"es-2SbE52__O","outputId":"0f344518-8d99-46e9-879c-ca74d2bffb2c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(1., grad_fn=<SumBackward0>)"]},"metadata":{},"execution_count":84}],"source":["softmax = torch.nn.Softmax(dim=-1)\n","torch.sum(softmax(output))"]},{"cell_type":"markdown","metadata":{"id":"s9aXrzRCKQel"},"source":["### Train"]},{"cell_type":"markdown","metadata":{"id":"L5tYJoeKMmuz"},"source":["#### Hyperparameters and training configuration"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UxbOAYCDLlrd"},"outputs":[],"source":["num_epochs: int = 5\n","learning_rate: float = 5e-2\n","optimizer: torch.optim = torch.optim.Adam(cbow_model.parameters(),\n","                                          lr=learning_rate)\n","\n","loss_function = nn.NLLLoss()"]},{"cell_type":"markdown","metadata":{"id":"vgn6aHDsWk2r"},"source":["#### Training phase"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":53,"status":"ok","timestamp":1743754827985,"user":{"displayName":"Khiêm Đặng Lê","userId":"14941239270249401110"},"user_tz":-420},"id":"2Bv0CVQZKS7W","outputId":"2a6b8649-dfd8-481d-c838-6473f3ff8086"},"outputs":[{"output_type":"stream","name":"stdout","text":["#Epoch 1/5\n","Loss: 4.489914894104004\n","#Epoch 2/5\n","Loss: 3.731325626373291\n","#Epoch 3/5\n","Loss: 3.085339307785034\n","#Epoch 4/5\n","Loss: 2.4373152256011963\n","#Epoch 5/5\n","Loss: 1.6826579570770264\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-86-38886483083f>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_vector, target_vector = torch.tensor(make_context_vector(data[0][0], word_to_idx)), torch.tensor(word_to_idx[data[0][1]])\n","<ipython-input-86-38886483083f>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_tensor = torch.tensor(make_context_vector(data[idx][0], word_to_idx)).unsqueeze(0)\n"]}],"source":["for epoch in range(1, num_epochs + 1):\n","    print(f\"#Epoch {epoch}/{num_epochs}\")\n","\n","    # Construct input and target tensor\n","    input_vector, target_vector = torch.tensor(make_context_vector(data[0][0], word_to_idx)), torch.tensor(word_to_idx[data[0][1]])\n","    input_vector = input_vector.unsqueeze(0)\n","    target_vector = target_vector.unsqueeze(0)\n","\n","    # Join whole data into 1 tensor set\n","    for idx in range(1, len(data)):\n","        input_tensor = torch.tensor(make_context_vector(data[idx][0], word_to_idx)).unsqueeze(0)\n","        target_tensor = torch.tensor(word_to_idx[data[idx][1]]).unsqueeze(0)\n","        input_tensor = torch.cat((input_vector, input_tensor), 0)\n","        target_tensor = torch.cat((target_vector, target_tensor), 0)\n","\n","    # Zero out the gradients from the old instance to avoid tensor accumulation\n","    cbow_model.zero_grad()\n","\n","    # Forward passing\n","    log_probabilities = cbow_model(input_vector)\n","\n","    # Evaluate loss\n","    loss = loss_function(log_probabilities, target_vector)\n","\n","    # Backpropagation\n","    loss.backward()\n","\n","    # Update the gradient according to the optimization algorithm\n","    optimizer.step()\n","\n","    # Get loss values\n","    epoch_loss = loss.item()\n","    print(\"Loss:\", epoch_loss)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1743754828009,"user":{"displayName":"Khiêm Đặng Lê","userId":"14941239270249401110"},"user_tz":-420},"id":"9wK3kiBd5CLa","outputId":"6f573c80-8e7b-488e-f095-2493bdfe3497"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 1,  4, 22, 37]])\n"]}],"source":["print(input_vector)"]},{"cell_type":"markdown","metadata":{"id":"7DrF_htiSXmI"},"source":["#### Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1743754828017,"user":{"displayName":"Khiêm Đặng Lê","userId":"14941239270249401110"},"user_tz":-420},"id":"L3SBZo4LSDP5","outputId":"c50d09ac-9292-4c9a-9f5a-36ab22e9d2fd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Context: ['In', 'processes.', 'we', 'conjure']\n","Prediction: about\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-88-149b12a03e6d>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_tensor = torch.tensor(make_context_vector(context, word_to_idx)).unsqueeze(0)\n"]}],"source":["with torch.no_grad(): # No gradient update in inference\n","    context = ['In', 'processes.', 'we', 'conjure']\n","\n","    # Vectorize input from text to numeric type\n","    input_tensor = torch.tensor(make_context_vector(context, word_to_idx)).unsqueeze(0)\n","\n","    # Model makes prediction\n","    output_tensor = cbow_model(input_tensor)\n","\n","    # Get the item id with the highest probability\n","    prediction = torch.argmax(output_tensor).detach().tolist()\n","\n","    # Query the respective word from the given item id\n","    key_list = list(word_to_idx.keys())\n","    prediction = key_list[prediction]\n","\n","    print(\"Context:\", context)\n","    print(\"Prediction:\", prediction)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1743754828030,"user":{"displayName":"Khiêm Đặng Lê","userId":"14941239270249401110"},"user_tz":-420},"id":"_bp9-1lj4Lj3","outputId":"d4be781d-65f0-4540-8f7f-282f33b4292e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Context: ['As', 'computers.', 'evolve,', 'processes']\n","Prediction: People\n","Ground truth:  they\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-89-9bc5e4be77c6>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  input_tensor = torch.tensor(make_context_vector(context, word_to_idx)).unsqueeze(0)\n"]}],"source":["with torch.no_grad(): # No gradient update in inference\n","    idx = 18\n","    context = data[idx][0]\n","    ground_truth = data[idx][1]\n","\n","    # Vectorize input from text to numeric type\n","    input_tensor = torch.tensor(make_context_vector(context, word_to_idx)).unsqueeze(0)\n","\n","    # Model makes prediction\n","    output_tensor = cbow_model(input_tensor)\n","\n","    # Get the item id with the highest probability\n","    prediction = torch.argmax(output_tensor).detach().tolist()\n","    # Query the respective word from the given item id\n","    key_list = list(word_to_idx.keys())\n","    prediction = key_list[prediction]\n","\n","    print(\"Context:\", context)\n","    print(\"Prediction:\", prediction)\n","    print(\"Ground truth: \", ground_truth)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1743754828031,"user":{"displayName":"Khiêm Đặng Lê","userId":"14941239270249401110"},"user_tz":-420},"id":"mTs44R4J4Wqt","outputId":"0b2a95c5-de5a-42c2-eecf-a57a105c242a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'inhabit'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":90}],"source":["data[15][1]"]},{"cell_type":"markdown","metadata":{"id":"X9FwHWmGaM3S"},"source":["## Skip-gram\n","\n","<center>\n","<img src=\"https://machinelearningcoban.com/tabml_book/_images/word2vec2.png\">\n","</center>\n","\n","- Skip gram is based on the distributional hypothesis where words with similar distribution is considered to have similar meanings. Researchers of skip gram suggested a model with less parameters along with the novel methods to make optimization step more efficient.\n","\n","- Vanilla SkipGram model:\n","\n","<center>\n","<img src=\"https://d3i71xaburhd42.cloudfront.net/a1d083c872e848787cb572a73d97f2c24947a374/5-Figure1-1.png\" scale=70%>\n","</center>\n","\n","- Main idea is to optimize model so that if it is queried with a word, it should correctly guess all the context (context = 2 in the figure) words. That is,\n","$$\n","y=\\sigma(Ux)\n","$$\n","    - where $x$, $y$ are one-hot encoded word vector, $U$ is the embedding matrix, and $\\sigma(\\cdot)$ is the softmax function.\n","\n","With the same dataset, training set for skip gram can be much larger than that of NPLM since it can have $2c$ samples $\\left(w_t:w_{t-c}, ...,w_t:w_{t-1},w_t:w_{t+1},...,w_{t+c}\\right)$ while other n-gram based models have one $\\left((w_{t-c},...w_{t-1},w_{t+1},...,w_{t+c}):w_t\\right)$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b9XRCWEvEOWC"},"outputs":[],"source":["corpus = \"\"\"We are about to study the idea of a computational process.\n","Computational processes are abstract beings that inhabit computers.\n","As they evolve, processes manipulate other abstract things called data.\n","The evolution of a process is directed by a pattern of rules\n","called a program. People create programs to direct processes. In effect,\n","we conjure the spirits of the computer with our spells.\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S1dwgXNxZ_Lv"},"outputs":[],"source":["class SkipGramModel(nn.Module):\n","    def __init__(self,\n","                 vocab_size: int,\n","                 embed_dim: int) -> None:\n","        \"\"\"\n","        Model construction\n","        \"\"\"\n","        super().__init__()\n","        self.vocab_size = vocab_size\n","        self.embed_dim = embed_dim\n","\n","        ### START YOUR CODE HERE ###\n","        # Declare embedding function u and v\n","        # with given vocab size and embed dim using nn.Embedding\n","        self.v_embedding_layer = nn.Embedding(self.vocab_size, self.embed_dim)\n","        self.u_embedding_layer = nn.Embedding(self.vocab_size, self.embed_dim)\n","\n","        # Network weight initialization with Xavier initialization\n","        nn.init.xavier_normal_(self.u_embedding_layer.weight)\n","        nn.init.xavier_normal_(self.v_embedding_layer.weight)\n","\n","        ### END YOUR CODE HERE ###\n","\n","    def forward(self, center_words, context):\n","        \"\"\"\n","        Function to perform forward passing\n","        \"\"\"\n","        v_embedding = self.v_embedding_layer(center_words)\n","        u_embedding = self.u_embedding_layer(context)\n","\n","        score = torch.mul(v_embedding, u_embedding)\n","        score = torch.sum(score, dim=1)\n","        log_score = F.logsigmoid(score)\n","        return log_score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1743754828058,"user":{"displayName":"Khiêm Đặng Lê","userId":"14941239270249401110"},"user_tz":-420},"id":"W4ls_wI-7nS1","outputId":"85f9a42f-72ca-45b6-8cbd-1a883a1dd444"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["SkipGramModel(\n","  (v_embedding_layer): Embedding(49, 128)\n","  (u_embedding_layer): Embedding(49, 128)\n",")"]},"metadata":{},"execution_count":94}],"source":["skipgram_model = SkipGramModel(vocab_size=vocab_size,\n","                               embed_dim=128)\n","\n","skipgram_model.train()\n","skipgram_model"]},{"cell_type":"markdown","metadata":{"id":"oiBCaJJJCMr_"},"source":["### Prepare training data to match the format of SkipGram model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5YlRPTuaCQBS"},"outputs":[],"source":["def gather_training_data(corpus,\n","                         word_to_idx: dict,\n","                         context_size: int):\n","    \"\"\"\n","    This function is to transform the given corpus\n","    into the correct format for SkipGram to serve as its input\n","    \"\"\"\n","\n","    training_data = []\n","    all_vocab_indices = list(range(len(word_to_idx)))\n","\n","    split_text = corpus.split('\\n')\n","\n","    # For each sentence\n","    for sentence in split_text:\n","        indices = []\n","        indices = [word_to_idx[word] for word in sentence.split(' ')]\n","\n","        # For each word treated as center word\n","        for center_word_pos in range(len(indices)):\n","\n","            # For each window  position\n","            for w in range(-context_size, context_size+1):\n","                context_word_pos = center_word_pos + w\n","\n","                # Make sure we dont jump out of the sentence\n","                if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n","                    continue\n","\n","                context_word_idx = indices[context_word_pos]\n","                center_word_idx  = indices[center_word_pos]\n","\n","                # Same words might be present in the close vicinity of each other. we want to avoid such cases\n","                if center_word_idx == context_word_idx:\n","                    continue\n","\n","                training_data.append([center_word_idx, context_word_idx])\n","\n","    return training_data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1743754828060,"user":{"displayName":"Khiêm Đặng Lê","userId":"14941239270249401110"},"user_tz":-420},"id":"FVjk1oJkDPXq","outputId":"cc46e677-d2a6-4d64-c219-e9cabd1eeb25"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([212, 2])"]},"metadata":{},"execution_count":96}],"source":["training_data = gather_training_data(corpus,\n","                                     word_to_idx,\n","                                     context_size=2)\n","training_data = torch.tensor(training_data).to(dtype=torch.long)\n","training_data.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1743754828061,"user":{"displayName":"Khiêm Đặng Lê","userId":"14941239270249401110"},"user_tz":-420},"id":"lHRbGjzLs_jJ","outputId":"bf017d1e-152b-4ca6-ad69-63d2c7f25c4a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 4,  1],\n","        [ 4, 10],\n","        [ 1,  4],\n","        [ 1, 10],\n","        [ 1, 22],\n","        [10,  4],\n","        [10,  1],\n","        [10, 22],\n","        [10, 37],\n","        [22,  1],\n","        [22, 10],\n","        [22, 37],\n","        [22, 15],\n","        [37, 10],\n","        [37, 22],\n","        [37, 15],\n","        [37, 30],\n","        [15, 22],\n","        [15, 37],\n","        [15, 30],\n","        [15, 23],\n","        [30, 37],\n","        [30, 15],\n","        [30, 23],\n","        [30,  9],\n","        [23, 15],\n","        [23, 30],\n","        [23,  9],\n","        [23, 31],\n","        [ 9, 30],\n","        [ 9, 23],\n","        [ 9, 31],\n","        [ 9, 48],\n","        [31, 23],\n","        [31,  9],\n","        [31, 48],\n","        [48,  9],\n","        [48, 31],\n","        [ 5, 35],\n","        [ 5,  1],\n","        [35,  5],\n","        [35,  1],\n","        [35, 36],\n","        [ 1,  5],\n","        [ 1, 35],\n","        [ 1, 36],\n","        [ 1, 20],\n","        [36, 35],\n","        [36,  1],\n","        [36, 20],\n","        [36,  6],\n","        [20,  1],\n","        [20, 36],\n","        [20,  6],\n","        [20, 29],\n","        [ 6, 36],\n","        [ 6, 20],\n","        [ 6, 29],\n","        [ 6, 47],\n","        [29, 20],\n","        [29,  6],\n","        [29, 47],\n","        [47,  6],\n","        [47, 29],\n","        [44, 46],\n","        [44, 39],\n","        [46, 44],\n","        [46, 39],\n","        [46, 35],\n","        [39, 44],\n","        [39, 46],\n","        [39, 35],\n","        [39, 41],\n","        [35, 46],\n","        [35, 39],\n","        [35, 41],\n","        [35, 12],\n","        [41, 39],\n","        [41, 35],\n","        [41, 12],\n","        [41, 36],\n","        [12, 35],\n","        [12, 41],\n","        [12, 36],\n","        [12,  0],\n","        [36, 41],\n","        [36, 12],\n","        [36,  0],\n","        [36,  2],\n","        [ 0, 12],\n","        [ 0, 36],\n","        [ 0,  2],\n","        [ 0, 33],\n","        [ 2, 36],\n","        [ 2,  0],\n","        [ 2, 33],\n","        [33,  0],\n","        [33,  2],\n","        [14, 11],\n","        [14, 23],\n","        [11, 14],\n","        [11, 23],\n","        [11,  9],\n","        [23, 14],\n","        [23, 11],\n","        [23,  9],\n","        [23, 43],\n","        [ 9, 11],\n","        [ 9, 23],\n","        [ 9, 43],\n","        [ 9, 24],\n","        [43, 23],\n","        [43,  9],\n","        [43, 24],\n","        [43, 34],\n","        [24,  9],\n","        [24, 43],\n","        [24, 34],\n","        [24, 18],\n","        [34, 43],\n","        [34, 24],\n","        [34, 18],\n","        [34,  9],\n","        [18, 24],\n","        [18, 34],\n","        [18,  9],\n","        [18,  7],\n","        [ 9, 34],\n","        [ 9, 18],\n","        [ 9,  7],\n","        [ 9, 23],\n","        [ 7, 18],\n","        [ 7,  9],\n","        [ 7, 23],\n","        [ 7, 13],\n","        [23,  9],\n","        [23,  7],\n","        [23, 13],\n","        [13,  7],\n","        [13, 23],\n","        [ 2,  9],\n","        [ 2, 38],\n","        [ 9,  2],\n","        [ 9, 38],\n","        [ 9, 17],\n","        [38,  2],\n","        [38,  9],\n","        [38, 17],\n","        [38,  3],\n","        [17,  9],\n","        [17, 38],\n","        [17,  3],\n","        [17, 40],\n","        [ 3, 38],\n","        [ 3, 17],\n","        [ 3, 40],\n","        [ 3, 22],\n","        [40, 17],\n","        [40,  3],\n","        [40, 22],\n","        [40, 32],\n","        [22,  3],\n","        [22, 40],\n","        [22, 32],\n","        [22, 26],\n","        [32, 40],\n","        [32, 22],\n","        [32, 26],\n","        [32, 28],\n","        [26, 22],\n","        [26, 32],\n","        [26, 28],\n","        [26, 21],\n","        [28, 32],\n","        [28, 26],\n","        [28, 21],\n","        [21, 26],\n","        [21, 28],\n","        [19, 25],\n","        [19, 15],\n","        [25, 19],\n","        [25, 15],\n","        [25, 45],\n","        [15, 19],\n","        [15, 25],\n","        [15, 45],\n","        [15, 23],\n","        [45, 25],\n","        [45, 15],\n","        [45, 23],\n","        [45, 15],\n","        [23, 15],\n","        [23, 45],\n","        [23, 15],\n","        [23, 42],\n","        [15, 45],\n","        [15, 23],\n","        [15, 42],\n","        [15, 27],\n","        [42, 23],\n","        [42, 15],\n","        [42, 27],\n","        [42, 16],\n","        [27, 15],\n","        [27, 42],\n","        [27, 16],\n","        [27,  8],\n","        [16, 42],\n","        [16, 27],\n","        [16,  8],\n","        [ 8, 27],\n","        [ 8, 16]])"]},"metadata":{},"execution_count":97}],"source":["training_data"]},{"cell_type":"markdown","metadata":{"id":"tWEIXRlg8zmg"},"source":["### Hyperparamters and training configuration"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wIgEYDKz8Sbu"},"outputs":[],"source":["num_epochs: int = 200\n","learning_rate: float = 5e-1\n","optimizer: torch.optim = torch.optim.SGD(skipgram_model.parameters(),\n","                                          lr=learning_rate)"]},{"cell_type":"markdown","metadata":{"id":"NmG0m3jK84EF"},"source":["### Training phase"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1743754828088,"user":{"displayName":"Khiêm Đặng Lê","userId":"14941239270249401110"},"user_tz":-420},"id":"VhGy9vBq85PX","outputId":"f8379832-164b-42da-de59-ef09152c26d8"},"outputs":[{"output_type":"stream","name":"stdout","text":["#Epoch 0/200\n","Loss: 0.6919984817504883\n","#Epoch 50/200\n","Loss: 0.6037424206733704\n","#Epoch 100/200\n","Loss: 0.5197837948799133\n","#Epoch 150/200\n","Loss: 0.4365466237068176\n","#Epoch 200/200\n","Loss: 0.3578314185142517\n"]}],"source":["for epoch in range(num_epochs + 1):\n","    \"\"\"\n","    Adapt the given CBOW training code for SkipGram\n","    Following by the instruction comments, or you could do it on your own ;)\n","    \"\"\"\n","    ### START YOUR CODE HERE ###\n","\n","    # Construct input and target tensor\n","    inputs = training_data[:, 0]\n","    targets = training_data[:, 1]\n","\n","    # Zero out the gradients from the old instance to avoid tensor accumulation\n","    skipgram_model.zero_grad()\n","\n","    # Forward passing\n","    logsoftmax_prediction = skipgram_model(inputs, targets)\n","\n","    # Evaluate loss (Negative log likelihood)\n","    loss = torch.mean(-1 * logsoftmax_prediction)\n","\n","    # Backpropagation\n","    loss.backward()\n","\n","    # Update the gradient according to the optimization algorithm\n","    optimizer.step()\n","\n","    # Get loss values\n","    epoch_loss = loss.item()\n","\n","    # Log result\n","    if epoch % 50 == 0:\n","        print(f\"#Epoch {epoch}/{num_epochs}\")\n","        print(\"Loss:\", epoch_loss)\n","\n","    ### END YOUR CODE HERE ###"]},{"cell_type":"markdown","metadata":{"id":"-X8DX0BfFDsK"},"source":["### Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1743754828096,"user":{"displayName":"Khiêm Đặng Lê","userId":"14941239270249401110"},"user_tz":-420},"id":"au4vPlQ_y-dh","outputId":"a8fc79a7-695c-46f2-ef2f-6df762346ef5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'things': 0,\n"," 'are': 1,\n"," 'called': 2,\n"," 'create': 3,\n"," 'We': 4,\n"," 'Computational': 5,\n"," 'that': 6,\n"," 'pattern': 7,\n"," 'spells.': 8,\n"," 'a': 9,\n"," 'about': 10,\n"," 'evolution': 11,\n"," 'other': 12,\n"," 'rules': 13,\n"," 'The': 14,\n"," 'the': 15,\n"," 'our': 16,\n"," 'People': 17,\n"," 'by': 18,\n"," 'we': 19,\n"," 'beings': 20,\n"," 'effect,': 21,\n"," 'to': 22,\n"," 'of': 23,\n"," 'is': 24,\n"," 'conjure': 25,\n"," 'processes.': 26,\n"," 'with': 27,\n"," 'In': 28,\n"," 'inhabit': 29,\n"," 'idea': 30,\n"," 'computational': 31,\n"," 'direct': 32,\n"," 'data.': 33,\n"," 'directed': 34,\n"," 'processes': 35,\n"," 'abstract': 36,\n"," 'study': 37,\n"," 'program.': 38,\n"," 'evolve,': 39,\n"," 'programs': 40,\n"," 'manipulate': 41,\n"," 'computer': 42,\n"," 'process': 43,\n"," 'As': 44,\n"," 'spirits': 45,\n"," 'they': 46,\n"," 'computers.': 47,\n"," 'process.': 48}"]},"metadata":{},"execution_count":100}],"source":["word_to_idx"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":73,"status":"ok","timestamp":1743755352145,"user":{"displayName":"Khiêm Đặng Lê","userId":"14941239270249401110"},"user_tz":-420},"id":"eD6c5ooPqQa1","outputId":"297a38b3-b707-46c9-9c38-9c51eb63c97f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Context: ['we']\n","Prediction: ['study', 'the', 'spirits', 'with']\n"]}],"source":["with torch.no_grad():\n","    context = ['we'] # center word\n","\n","    ### START YOUR CODE HERE ###\n","    # Based on the given inference code in the previous section, training code and the context\n","    # Implement the inference flow from the given context to an output word\n","\n","    # Chọn ma trận embedding là ma trận V\n","    embedding_matrix = skipgram_model.v_embedding_layer.weight\n","\n","    # Embedding từ đầu vào\n","    input_tensor = torch.tensor(word_to_idx[context[0]]).unsqueeze(0)\n","    input_embedding = embedding_matrix[input_tensor]\n","\n","    # Sử dụng log_softmax thay thế cho soft_max\n","    predict_matrix = F.log_softmax(input_embedding @ embedding_matrix.T, dim=1)\n","\n","    context_size = 2\n","    # Sử dụng ma trận embedding để tìm các từ tương đồng\n","    top_indices = torch.topk(predict_matrix, 2 * context_size + 1, sorted=True).indices\n","\n","    # IN kết quả\n","    key_list = list(word_to_idx.keys())\n","    prediction = [key_list[idx] for idx in top_indices[0]]\n","    prediction = prediction[1:]\n","\n","\n","    print(\"Context:\", context)\n","    print(\"Prediction:\", prediction)"]},{"cell_type":"markdown","metadata":{"id":"6sg4EaBpw-De"},"source":["## Problem 3\n","What are the differences between CBOW and Skip-gram?"]},{"cell_type":"markdown","metadata":{"id":"kYNYm8-wQlfK"},"source":["- Mục đích của CBOW là dự đoán từ *target* dựa vào các từ *context* lân cận và mục đích của Skip-gram là dự đoán các từ *context* dựa vào từ target. Do đó, đầu vào của mô hình CBOW là một danh sách các từ lân cận và đầu ra là một từ target duy nhất. Ngược lại, đầu vào của Skip-gram là một từ target duy nhất và đầu ra là các từ lân cận.\n","- Trong mô hình CBOW, ngoài ma trận embedding còn có thêm ma trận trọng số được thể hiện trong lớp Linear. Ma trận này có nhiệm vụ biến đổi từ ma trận embedding tổng hợp đầu vào thành ma trận kết quả. Còn trong mô hình Skip-gram, ta chỉ có 2 ma trận embedding từ cho từ context và từ trung tâm."]}],"metadata":{"colab":{"collapsed_sections":["w303qoG_2pkh","bQPQFfOu0yFA"],"provenance":[{"file_id":"1Elz7qYimnDWrWAXDH0kwhwwnoidqkAZA","timestamp":1743500511488},{"file_id":"1PBjNxDsAjbb_Q_aLhX8nfo_BaoSxAyji","timestamp":1713178011688}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}